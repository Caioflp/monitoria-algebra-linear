\documentclass[leqno]{article}

\usepackage[brazil]{babel}% \usepackage[latin1]{inputenc}
\usepackage{a4wide}
\setlength{\oddsidemargin}{-0.2in}
% % \setlength{\oddsidemargin}{0.2in}
\setlength{\evensidemargin}{-0.2in}
% % \setlength{\evensidemargin}{0.5in}
% % \setlength{\textwidth}{5.5in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-1.2in}
\setlength{\textheight}{10in}
\usepackage[]{amsfonts} \usepackage[]{amsmath}
\usepackage[]{amssymb} \usepackage[]{latexsym}
\usepackage{graphicx,color} \usepackage{amsthm}
\usepackage{mathrsfs} \usepackage{url}
\usepackage{cancel} \usepackage{enumerate}
\usepackage{xifthen} \usepackage{tikz}
\usepackage{mathtools} 
\usepackage{hyperref}
\usetikzlibrary{automata,arrows,positioning,calc}

\input{../preamble}

\numberwithin{equation}{section}

\setlength{\parindent}{12 pt}


%% \newtheorem{teo}{Teorema}[section] \newtheorem*{teo*}{Teorema}
%% \newtheorem{prop}[teo]{Proposição} \newtheorem*{prop*}{Proposição}
%% \newtheorem{lema}[teo]{Lemma} \newtheorem*{lema*}{Lema}
%% \newtheorem{cor}[teo]{Corolário} \newtheorem*{cor*}{Corolário}

%% \theoremstyle{definition}
%% \newtheorem{defi}[teo]{Definição} \newtheorem*{defi*}{Definição}
%% \newtheorem{exem}[teo]{Exemplo} \newtheorem*{exem*}{Exemplo}
%% \newtheorem{obs}[teo]{Observação} \newtheorem*{obs*}{Observação}
%% \newtheorem*{hipo}{Hipóteses}
%% \newtheorem*{nota}{Notação}

\newcommand{\ds}{\displaystyle} \newcommand{\nl}{\newline}
\newcommand{\eps}{\varepsilon} \newcommand{\ssty}{\scriptstyle}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cLN}{\mathcal{LN}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bR}{\mathbb{R}}
\newcommand{\bZ}{\mathbb{Z}} 

\newcommand{\bfw}{\mathbf{w}}
\newcommand{\bfv}{\mathbf{v}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfz}{\mathbf{z}}
\newcommand{\bfe}{\mathbf{e}}
\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\bfx}{\mathbf{x}}


\newcommand{\bvecc}[2]{%
    \begin{bmatrix} #1 \\ #2  \end{bmatrix}
}
\newcommand{\bveccc}[3]{%
    \begin{bmatrix} #1 \\ #2 \\ #3  \end{bmatrix}
}

\newcommand{\bvecfour}[4]{%
    \begin{bmatrix} #1 \\ #2 \\ #3 \\ #4 \end{bmatrix}
}

\newenvironment{sol}
{
    \vspace{4mm}
    \noindent\textbf{Resolução:}
    \strut\newline
    \smallskip
    \hspace{-3.5mm}
}
{} 

\title{Álgebra Linear - Soluções da Lista de Exercícios 5}

\author{Caio Lins e Tiago Silva}
\date{\today} 

\begin{document}

\maketitle

\begin{rem*}
    Escreveremos \( ( x_{ 1 }, \dots, x_{ n } ) \), com parênteses, para denotar um vetor em \( \R^{ n } \) cujas coordenadas na base canônica são \( x_{ 1 }, \dots, x_{ n } \).
    Ou seja, \( ( x_{ 1 }, \dots, x_{ n } ) \) é para ser entendido como a mesma coisa que
    \begin{equation*}
        \begin{bmatrix}
            x_{ 1 } \\
            \vdots \\
            x_{ n }
        \end{bmatrix}
    .\end{equation*}
    Utilizaremos essa notação para evitar o uso de transposição ao definir um vetor.
    Ao invés de escrevermos \( \bfv =
    \begin{bmatrix}
        x_{ 1 } & \cdots & x_{ n }
    \end{bmatrix}^{ \transpose }\), podemos agora escrever apenas \( \bfv = ( x_{ 1 }, \dots, x_{ n } ) \).
\end{rem*}

\begin{enumerate}

    \item Explique porque essas afirmações são falsas

        \begin{enumerate}

            \item A solução completa é qualquer combinação linear de $x_p$ e $x_n$.

                \begin{sol} 
                    Se o sistema \( Ax = b \) tem \( b = 0 \), isso é verdade, pois qualquer solução particular está no núcleo de \( A \), o qual é um espaço vetorial.
                    Se, por outro lado, tivermos \( b \neq 0 \), a afirmação está errada.
                    De fato, tome a combinação linear de \( x_{ p } \) e \( x_{ n } \) dada por \( 2 \cdot x_{ p } + 0 \cdot x_{ n } \).
                    Então
                    \begin{equation*}
                        A ( 2x_{ p } ) = 2 Ax_{ p } = 2b \neq b
                    .\end{equation*}
                    Logo, ela não é uma solução.
                \end{sol} 

            \item O sistema $Ax = b$ tem no máximo uma solução particular.

                \begin{sol} 
                    Se há alguma solução particular e o núcleo de \( A \) é não vazio, então há infinitas soluções.
                \end{sol} 

            \item Se $A$ é inversível, não existe nenhuma solução $x_n$ no núcleo.

                \begin{sol} 
                    Na verdade, sempre existe a solução trivial \( x = 0 \) no núcleo.
                \end{sol} 

        \end{enumerate}

    \item Sejam
        $$U = \begin{bmatrix} 
            1 & 2 & 3 \\
            0 & 0 & 4
            \end{bmatrix} \mbox{ e } c = \begin{bmatrix} 
            5 \\
            8
        \end{bmatrix}.$$

        Use a eliminação de Gauss-Jordan para reduzir as matrizes $[U \ 0]$ e $[U \ c]$ para $[R \ 0]$ e $[R \ d]$. Resolva $Rx = 0$ e $Rx = d$

        \begin{sol} 
            Vamos trabalhar de uma vez com a matriz aumentada \( [ U \ c] \).
            \begin{align*}
                \begin{bmatrix}
                    1 & 2 & 3 & \mid & 5 \\
                    0 & 0 & 4 & \mid & 8
                \end{bmatrix}
                &\xrightarrow{
                    \begin{array}{l}
                        L_{ 1 } - \frac{ 3 }{ 4 } L_{ 2 }
                    \end{array}
                }
                \begin{bmatrix}
                    1 & 2 & 0 & \mid & -1 \\
                    0 & 0 & 4 & \mid & 8
                \end{bmatrix} \\
                &\xrightarrow{
                    \begin{array}{l}
                        L_{ 2 }/4
                    \end{array}
                }
                \begin{bmatrix}
                    1 & 2 & 0 & \mid & -1 \\
                    0 & 0 & 1 & \mid & 2
                \end{bmatrix}
            .\end{align*}
            Como \( U \) possui \( 2 \) pivôs, pelo Teorema do Posto sabemos que seu núcleo tem dimensão \( 1 \).
            Como a segunda coluna não tem pivô, a variável livre é \( x_{ 2 } \).
            Portanto, para encontrar uma base para o núcleo podemos definir \( x_{ 2 } = 1 \) e, com isso, concluímos que \( x_{ 1 } = -2 \) e \( x_{ 3 } = 0 \).
            Ou seja, a solução de \( Rx = 0 \) é o \( \vspan \) do vetor \( ( -2, 1, 0 ) \).

            Para obter uma solução particular de \( Rx = d \), novamente definimos \( x_{ 2 } = 1 \) e, dessa vez, obtemos \( x_{ 1 } = -3 \) e \( x_{ 3 } = 2 \).
            Logo, a solução completa de \( Rx = d \) é dada por
            \begin{equation*}
                ( -3, 1, 2 ) + t ( -2, 1, 0 ), t \in \R
            .\end{equation*}
        \end{sol} 

    \item Suponha que $Ax = b$ e $Cx = b$ tenham as mesmas soluções (completas) para todo $b$. Podemos concluir que $A = C$?

        \begin{sol} 
            Podemos.
            Seja \( \bfe_{ i } \) o \( i \)-ésimo vetor da base canônica e defina \( \bfa_{ i } = A \bfe_{ i } \), ou seja, \( \bfa_{ i } \) é a \( i \)-ésima coluna de \( A \).
            Por hipótese, \( \bfe_{ i } \) deve ser solução do sistema \( C \bfx = \bfa_{ i } \), porém isso implica em \( C \bfe_{ i } = \bfa_{ i } \), ou seja, a \( i \)-ésima coluna de \( C \) é igual à \( i \)-ésima coluna de \( A \).
            Como \( i \) foi tomado arbitrariamente, \( A = C \).
        \end{sol} 

    \item Ache o maior número possível de vetores linearmente independentes dentre os vetores:

        $$\bvecfour{1}{-1}{0}{0}, \ \bvecfour{1}{0}{-1}{0}, \ \bvecfour{1}{0}{0}{-1}, \ \bvecfour{0}{1}{-1}{0}, \ \bvecfour{0}{1}{0}{-1} \mbox{ e } \bvecfour{0}{0}{1}{-1}$$

        \begin{sol} 
            Vamos realizar a eliminação de Gauss-Jordan em uma matriz que tem esses vetores como colunas.
            Com isso, obteremos o posto dessa matriz, ou seja, o maior número de colunas L.I. que ela possui.
            \begin{align*}
                \begin{bmatrix}
                    1 & 1 & 1 & 0 & 0 & 0 \\
                    -1 & 0 & 0 & 1 & 1 & 0 \\
                    0 & -1 & 0 & -1 & 0 & 1 \\
                    0 & 0 & -1 & 0 & -1 & -1
                \end{bmatrix}
                &\xrightarrow{
                    \begin{array}{l}
                        L_{ 2 } + L_{ 1 }
                    \end{array}
                }
                \begin{bmatrix}
                    1 & 1 & 1 & 0 & 0 & 0 \\
                    0 & 1 & 1 & 1 & 1 & 0 \\
                    0 & -1 & 0 & -1 & 0 & 1 \\
                    0 & 0 & -1 & 0 & -1 & -1
                \end{bmatrix} \\
                &\xrightarrow{
                    \begin{array}{l}
                        L_{ 3 } + L_{ 2 }
                    \end{array}
                }
                \begin{bmatrix}
                    1 & 1 & 1 & 0 & 0 & 0 \\
                    0 & 1 & 1 & 1 & 1 & 0 \\
                    0 & 0 & 1 & 0 & 1 & 1 \\
                    0 & 0 & -1 & 0 & -1 & -1
                \end{bmatrix} \\
                &\xrightarrow{
                    \begin{array}{l}
                        L_{ 4 } + L_{ 3 }
                    \end{array}
                }
                \begin{bmatrix}
                    1 & 1 & 1 & 0 & 0 & 0 \\
                    0 & 1 & 1 & 1 & 1 & 0 \\
                    0 & 0 & 1 & 0 & 1 & 1 \\
                    0 & 0 & 0 & 0 & 0 & 0
                \end{bmatrix}
            .\end{align*}
            Portanto, essa matriz tem \( 3 \) pivôs e, assim, esse é o maior número possível de vetores linearmente independentes dentre suas colunas.
        \end{sol} 

    \item Ache uma base para o plano $x - 2y + 3z = 0$ em $\bR^3$. Encontre então uma base para a interseção desse plano com o plano $xy$. Ache ainda uma base para todos os vetores perpendiculares a esse plano.

        \begin{sol} 
            Seja \( \Pi \) o plano em questão.
            Observe que esse plano é justamente o núcleo da matriz
            \begin{equation*}
                M =
                \begin{bmatrix}
                    1 & -2 & 3
                \end{bmatrix}
            .\end{equation*}
            Como ela já está em sua forma escalonada reduzida, podemos obter seu núcleo diretamente, pois ele será o \( \vspan  \) dos vetores
            \begin{equation*}
                \bfv =
                \begin{bmatrix}
                    2 \\
                    1 \\
                    0
                \end{bmatrix}
                \text{ e }
                \bfw =
                \begin{bmatrix}
                    -3 \\
                    0 \\
                    1
                \end{bmatrix}
            .\end{equation*}
            Sendo assim, \( \left\{ \bfv, \bfw \right\} \) é uma base para \( \Pi \).

            Os vetores \( ( x, y, z ) \in \R^{ 3 } \) pertencentes à interseção entre \( \Pi \) e o plano \( xy \) são justamente os que satisfazem \( x - 2y = 0 \), ou seja,
            \begin{equation*}
                x = 2y
            .\end{equation*}
            Portanto, eles são da forma \( ( 2t, t ), t \in \R \).
            É evidente que o vetor \( ( 2, 1 ) \) constitui uma base para esse conjunto.

            Agora, afirmamos que um vetor é perpendicular a \( \Pi \) se, e somente se, ele é perpendicular a \( \bfv \) e \( \bfw \).
            De fato, como \( \bfv \) e \( \bfw \) pertencem a \( \Pi \), naturalmente um vetor perpendicular a \( \Pi \) será perpendicular a \( \bfv \) e \( \bfw \).
            Reciprocamente, se \( \bfu \) é tal que \( \dotprod{\bfu, \bfv} = \dotprod{\bfu, \bfw} = 0 \), então, dado \( \bfx = \alpha \bfv + \beta \bfw \in \Pi \), temos
            \begin{align*}
                \dotprod{\bfu, \bfx} &= \dotprod{\bfu, \alpha \bfv + \beta \bfw} \\
                                     &= \alpha \dotprod{\bfu, \bfv} + \beta \dotprod{\bfu, \bfw} \\
                                     &= \alpha \cdot 0 + \beta \cdot 0 \\
                                     &= 0
                                 .\end{align*}
                             \end{sol} 
                             Portanto, o conjunto dos vetores perpendiculares a \( \Pi \) será justamente o núcleo da matriz
                             \begin{equation*}
                                 A =
                                 \begin{bmatrix}
                                     \bfv & \bfw
                                 \end{bmatrix}^{ \transpose }
                                 =
                                 \begin{bmatrix}
                                     \bfv^{ \transpose } \\
                                     \bfw^{ \transpose }
                                 \end{bmatrix}
                                 =
                                 \begin{bmatrix}
                                     2 & 1 & 0 \\
                                     -3 & 0 & 1
                                 \end{bmatrix}
                             .\end{equation*}
                             Vamos obter a forma escalonada reduzida de \( A \):
                             \begin{align*}
                                 \begin{bmatrix}
                                     2 & 1 & 0 \\
                                     -3 & 0 & 1
                                 \end{bmatrix}
            &\xrightarrow{
                \begin{array}{l}
                    L_{ 2 } + \frac{ 3 }{ 2 } L_{ 1 }
                \end{array}
            }
            \begin{bmatrix}
                2 & 1 & 0 \\
                0 & \frac{ 3 }{ 2 } & 1
            \end{bmatrix} \\
            &\xrightarrow{
                \begin{array}{l}
                    L_{ 1 } - \frac{ 2 }{ 3 } L_{ 2 }
                \end{array}
            }
            \begin{bmatrix}
                2 & 0 & - \frac{ 2 }{ 3 } \\
                0 & \frac{ 3 }{ 2 } & 1
            \end{bmatrix} \\
            &\xrightarrow{
                \begin{array}{l}
                    L_{ 1 } \cdot \frac{ 1 }{ 2 } \\
                    L_{ 2 } \cdot \frac{ 2 }{ 3 }
                \end{array}
            }
            \begin{bmatrix}
                1 & 0 & - \frac{ 1 }{ 3 } \\
                0 & 1 & \frac{ 2 }{ 3 }
            \end{bmatrix}
        .\end{align*}
        Logo, concluímos que o vetor \( ( \frac{ 1 }{ 3 }, - \frac{ 2 }{ 3 }, 1 ) \) constitui uma base para o núcleo de \( A \) e, assim, para o conjunto dos vetores perpendiculares a \( \Pi \).

    \item Ache (na sua forma mais simples) a matriz que é o produto das matrizes de posto 1 $\bfu \bfv^T$ e $\bfw \bfz^T$? Qual seu posto?

        \begin{sol} 
            Utilizando o fato de que \( \bfv^{ \transpose } \bfw \) é um número real, temos
            \begin{align*}
                ( \bfu \bfv^{ \transpose } ) ( \bfw \bfz^{ \transpose } )
                &= \bfu ( \bfv^{ \transpose } \bfw ) \bfz^{ \transpose } \\
                &= ( \bfv^{ \transpose } \bfw ) ( \bfu \bfz^{ \transpose } )
            .\end{align*}
            Sabemos que \( \bfu \bfz^{ \transpose } \) é uma matriz de posto \( 1 \).
            Logo, por ser múltipla de uma matriz de posto \( 1 \), \( ( \bfu \bfv^{ \transpose } ) ( \bfw \bfz^{ \transpose } ) \) tem posto \( 1 \).
        \end{sol} 

    \item Suponha que a coluna $j$ de $B$ é uma combinação linear das colunas anteriores de $B$. Mostre que a coluna $j$ de $AB$ é uma combinação linear das colunas anteriores de $AB$. Conclua que posto$(AB) \leq $ posto$(B)$.

        \begin{sol} 
            Vamos supor que \( A \) é \( m \times n \) e \( B \) é \( n \times p \).
            Sejam \( \bfa_{ 1 }, \dots, \bfa_{ n } \) as colunas de \( A \) e \( \bfb_{ 1 }, \dots, \bfb_{ p } \) as colunas de \( B \).
            Pelo enunciado, sabemos que existem \( \alpha_{ 1 }, \dots, \alpha_{ j-1 } \in \R \), não todos nulos, tais que
            \begin{equation*}
                \bfb_{ j } = \sum_{ i=1 }^{ j-1 } \alpha_{ i } \bfb_{ i }
            .\end{equation*}
            Multiplicando essa equação por \( A \) e usando a linearidade da multiplicação matricial, obtemos
            \begin{equation*}
                A \bfb_{ j } = \sum_{ i=1 }^{ j-1 } \alpha_{ i } ( A \bfb_{ i } )
            ,\end{equation*}
            ou seja, a \( j \)-ésima coluna de \( AB \) é combinação linear das colunas anteriores de \( AB \).

            Com isso, concluímos que o número de colunas L.D. de \( AB \) é maior ou igual ao número de colunas L.D. de \( B \).
            Como o posto de uma matriz é o maior número de colunas linearmente independentes que ela possui, isso implica \( \posto ( AB ) \leq \posto ( B ) \).
        \end{sol} 

    \item O item anterior nos dá posto$(B^T A^T) \leq $ posto$(A^T)$. É possível concluir que posto$(AB) \leq $ posto$(A)$?

        \begin{sol} 
            É possível.
            De fato, como \( \posto ( M^{ \transpose } ) = \posto ( M ) \) para toda matriz \( M \), temos
            \begin{equation*}
                \posto ( AB ) = \posto ( AB )^{ \transpose } = \posto ( B^{ \transpose } A^{ \transpose } ) \leq \posto ( A^{ \transpose } ) = \posto ( A )
            .\end{equation*}
        \end{sol} 

    \item Suponha que $A$ e $B$ são matrizes quadradas e $AB = I$. Prove que posto$(A) = n$. Conclua que $B$ precisa ser a inversa (de ambos lados) de $A$. Então, $BA = I$.

        \begin{sol} 
            Pela questão anterior, temos
            \begin{equation*}
                n = \posto ( I ) = \posto ( AB ) \leq \posto ( A )
            .\end{equation*}
            Como \( A \) é \( n \times n \), também temos \( \posto ( A ) \leq n \) e, com isso, \( \posto ( A ) = n \).
            Sendo assim, \( A \) é uma matriz invertível.
            Logo,
            \begin{equation*}
                B = IB = A^{ -1 } A B = A^{ -1 } I = A^{ -1 }
            .\end{equation*}
        \end{sol} 

    \item (\textit{Bônus}) Dado um espaço vetorial real \( V \), definimos o conjunto
        \begin{equation*}
            V^{ * } \defeq \left\{ f : V \to \R \mid f \text{ é linear} \right\}
        .\end{equation*}
        Ou seja, \( V^{ * } \) é o conjunto de todas as funções lineares entre \( V \) e \( \R \).
        Relembramos que uma função \( f : E \to F \), onde \( E \) e \( F \) são espaços vetoriais, é dita \textit{linear} se para todos \( \bfv, \bfw \in E \) e \( \alpha \in \R \) temos \( f ( \bfv + \bfw ) = f ( \bfv ) + f ( \bfw ) \) e \( f ( \alpha \bfv ) = \alpha f ( \bfv ) \).
        Chamamos \( V^{ * } \) de \textit{espaço dual} de \( V \).
        \begin{enumerate}
            \item Mostre que \( V^{ * } \) é um espaço vetorial.

                \begin{sol} 
                    Podemos definir a soma de funcionais (funções lineares que têm \( \R \) como contradomínio são chamadas de \textit{funcionais lineares}) de uma maneira natural.
                    Dadas \( f, g \in V^{ * } \), definimos
                    \begin{align*}
                        f + g : V &\to \R \\
                        \bfv &\mapsto f ( \bfv ) + g ( \bfv )
                    .\end{align*}
                \end{sol} 
                Com isso, a soma de dois funcionais lineares é um funcional linear .
                Além disso, definimos de maneira análoga a multiplicação de um funcional por um escalar.
                Dada \( f \in V^{ * } \) e \( \alpha \in \R \), definimos
                \begin{align*}
                    \alpha f : V &\to \R \\
                    \bfv &\mapsto \alpha f ( \bfv )
                .\end{align*}
                Portanto, a multiplicação de um funcional linear por um escalar é um funcional linear.

            \item Agora, seja \( V = \R^{ n } \).
                Mostre que existe uma bijeção \( \varphi : V^{ * } \to V \) tal que , para toda \( f \in V^{ * } \) e para todo \( \bfv \in V \), tenhamos
                \begin{equation*}
                    f(\bfv) = \dotprod{\varphi(f), \bfv}
                .\end{equation*}
                \textit{Dica}: Utilize a dimensão finita de \( \R^{ n } \) para expandir \( \bfv \) como uma combinação linear dos vetores da base canônica e aplique a linearidade de \( f \).

                \begin{sol} 
                    Denotanto por \( \bfe_{ i } \) o \( i \)-ésimo vetor da base canônica, dado \( \bfv \in V \) podemos escrever
                    \begin{equation*}
                        \bfv = \sum_{ i=1 }^{ n } \alpha_{ i } \bfe_{ i }
                    ,\end{equation*}
                    para alguns \( \alpha_{ i } \in \R \), \( i = 1, \dots, n \).
                    Com isso, se \( f \in V^{ * } \), pela sua linearidade podemos escrever
                    \begin{align*}
                        f ( \bfv )
                        &= f \left(
                            \sum_{ i=1 }^{ n } \alpha_{ i } \bfe_{ i }
                        \right) \\
                        &= \sum_{ i=1 }^{ n } f ( \alpha_{ i } \bfe_{ i } ) \\
                        &= \sum_{ i=1 }^{ n } \alpha_{ i } f ( \bfe_{ i } ) \\
                        &=
                        \begin{bmatrix}
                            f ( \bfe_{ 1 } ) & \cdots & f ( \bfe_{ n } )
                        \end{bmatrix}
                        \bfv
                    .\end{align*}
                    Perceba que, com isso, conhecendo os \( n \) valores \( f ( \bfe_{ i } ), \dots, f ( \bfe_{ n } ) \) podemos calcular \( f ( \bfv ) \) para qualquer vetor \( \bfv \in V \), realizando o produto interno
                    \begin{equation*}
                        \begin{bmatrix}
                            f ( \bfe_{ i } ) \\
                            \vdots \\
                            f ( \bfe_{ n } )
                        \end{bmatrix}^{ \transpose }
                        \bfv
                    .\end{equation*}
                    Sendo assim, podemos definir uma função
                    \begin{align*}
                        \varphi : V^{ * } &\to V \\
                        f &\mapsto \varphi ( f ) =
                        ( f ( \bfe_{ 1 } ), \dots, f ( \bfe_{ n } ) )
                    .\end{align*}
                    Pela discussão anterior, já temos a identidade
                    \begin{equation*}
                        f ( \bfv ) = \dotprod{\varphi ( f ), \bfv}
                    \end{equation*}
                    para toda \( f \in V^{ * } \) e \( \bfv \in V \).
                    Resta mostrar que \( \varphi \) é uma bijeção.

                    Vamos mostrar, primeiro, que \( \varphi \) é sobrejetiva.
                    Dado \( \bfu = ( \beta_{ 1 }, \dots, \beta_{ n } ) \in V\), devemos encontrar uma \( g \in V^{ * } \) tal que \( \varphi ( g ) = \bfu \).
                    Ora, definindo \( g \) por \( g ( \bfv ) = \bfu^{ \transpose } \bfv \) para todo \( \bfv \in V \), claramente \( g \in V^{ * } \) e, ainda,
                    \begin{equation*}
                        g ( \bfe_{ i } ) = \bfu^{ \transpose } \bfe_{ i } = \beta_{ i }
                    \end{equation*}
                    para todo \( i = 1, \dots, n \).
                    Logo, \( \varphi ( g ) = \bfu \).

                    Para mostrar a injetividade de \( \varphi \), vamos supor que existem \( f, g \in V^{ * } \ \) tais que \( \varphi ( f ) = \varphi ( g ) \).
                    Pela definição da \( \varphi \), isso implica em \( f ( \bfe_{ i } ) = g ( \bfe_{ i } ) \) para todo \( i = 1, \dots, n \).
                    Com isso, dado qualquer \( \bfv \in V \) com \( \bfv = ( \alpha_{ 1 }, \dots, \alpha_{ n } ) \), temos, por um desenvolvimento feito anteriormente,
                    \begin{equation*}
                        f ( \bfv )
                        = \sum_{ i=1 }^{ n } \alpha_{ i } f ( \bfe_{ i } )
                        = \sum_{ i=1 }^{ n } \alpha_{ i } g ( \bfe_{ i } )
                        = g ( \bfv )
                    .\end{equation*}
                    Portanto, \( f = g \) e \( \varphi \) é injetiva, o que conclui a demonstração. \hfill \qed
                \end{sol} 
        \end{enumerate}
        Em dimensão infinita, esse resultado é conhecido como \href{https://en.wikipedia.org/wiki/Riesz_representation_theorem}{Teorema da Representação de Riesz}.
\end{enumerate}

\end{document} 
